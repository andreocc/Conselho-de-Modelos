# üß† Conselho de Modelos Local (Local Model Council)

> **Orquestre m√∫ltiplos LLMs locais para debater, analisar e sintetizar solu√ß√µes complexas.**

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![Flask](https://img.shields.io/badge/Flask-Waitress-orange)
![Ollama](https://img.shields.io/badge/Ollama-Local%20AI-white)
![RAG](https://img.shields.io/badge/RAG-Local-green)

O **Conselho de Modelos Local** √© uma aplica√ß√£o web que permite consultar v√°rios Modelos de Linguagem (LLMs) rodando localmente via **Ollama** de forma paralela. Diferente de um chat comum, ele introduz um "Juiz" (Sintetizador) que analisa todas as respostas e entrega um veredito consolidado, eliminando alucina√ß√µes e enriquecendo a resposta final.

A aplica√ß√£o foi desenhada para privacidade total (100% offline), performance em hardware consumidor (Apple Silicon/Windows ARM64/x64) e usabilidade premium.

---

## ‚ú® Principais Funcionalidades

### ü§ñ Orquestra√ß√£o de M√∫ltiplos Modelos
- Selecione livremente quais modelos instalados no seu Ollama (Llama 3, Mistral, Gemma, Phi-3, etc.) far√£o parte do conselho.
- Execu√ß√£o paralela para minimizar o tempo de espera.

### ‚öñÔ∏è Sistema de Juiz e S√≠ntese
- Um modelo dedicado atua como "Presidente do Conselho".
- Ele l√™ todas as opini√µes individuais e gera um relat√≥rio final contendo: **Consensos**, **Diverg√™ncias** e uma **Conclus√£o Unificada**.

### üé≠ Personas do Conselho (Novo!)
Altere a din√¢mica do debate com modos predefinidos:
- **Debate (Opostos)**: For√ßa os modelos a assumirem pap√©is de *C√©tico*, *Vision√°rio* e *Pragm√°tico*.
- **Consultoria**: Foco em an√°lise t√©cnica e estruturada.
- **Criativo**: Brainstorming sem filtros.

### üìö RAG (Retrieval-Augmented Generation) Local
- **Docs**: Upload de PDFs, DOCX e TXT para dar contexto ao conselho.
- **Web**: Cole uma URL e o sistema ler√° o conte√∫do da p√°gina para embasar a discuss√£o.
- Tudo processado na mem√≥ria localmente (Embeddings via Ollama), sem envio de dados para nuvem.

### üé® Interface Premium
- UI moderna e responsiva (Dark Mode).
- Feedback em tempo real ("O Juiz est√° deliberando...").
- Hist√≥rico de sess√µes salvo localmente.

---

## üõ†Ô∏è Stack Tecnol√≥gica

- **Backend**: Python + Flask (transi√ß√£o de Streamlit para maior compatibilidade).
- **IA/LLM**: [Ollama](https://ollama.com/) (Biblioteca Python oficial).
- **Vetoriza√ß√£o**: NumPy + Ollama Embeddings (sem depend√™ncias pesadas como ChromaDB/Torch, ideal para ARM64).
- **Frontend**: HTML5, Vanilla CSS (Inter Font), JavaScript puro.

---

## üöÄ Como Executar

### Pr√©-requisitos
1.  Tenha o **[Ollama](https://ollama.com/)** instalado e rodando.
2.  Baixe alguns modelos (ex: `ollama pull llama3`, `ollama pull mistral`).
3.  Python 3 instalado.

### Instala√ß√£o R√°pida (Windows)

Basta executar o script autom√°tico:

```powershell
.\run_council.bat
```

O script ir√°:
1.  Criar um ambiente virtual (`venv`).
2.  Instalar as depend√™ncias (`flask`, `requests`, `beautifulsoup4`, etc.).
3.  Iniciar o servidor e abrir seu navegador em `http://127.0.0.1:8501`.

---

## üìñ Como Usar

1.  **Selecione os Conselheiros**: Marque as caixas dos modelos que deseja consultar na barra lateral.
2.  **Escolha o Juiz**: Defina qual modelo far√° a s√≠ntese final (recomendado um modelo mais capaz, como Llama 3 ou Mistral).
3.  **Defina o Contexto (Opcional)**:
    *   Fa√ßa upload de um arquivo PDF/DOCX.
    *   Ou cole uma URL para leitura.
4.  **Escolha a Persona**: Defina se quer um debate acalorado ou uma consultoria t√©cnica.
5.  **Pergunte**: Digite seu dilema e clique em "Convocar Conselho".

---

## üìÑ Licen√ßa

Este projeto √© open-source sob a licen√ßa [MIT](LICENSE). Sinta-se livre para modificar e distribuir.

---

Desenvolvido com foco em **Simplicidade**, **Privacidade** e **Poder Local**.
