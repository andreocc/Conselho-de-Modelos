# Local Model Council üß†

Uma aplica√ß√£o web Python que implementa um "Conselho de Modelos" usando Ollama. A aplica√ß√£o permite consultar m√∫ltiplos LLMs locais em paralelo e obter uma s√≠ntese consolidada, com suporte a RAG (Retrieval-Augmented Generation) para documentos.

Inspira√ß√£o: [Perplexity Model Council](https://www.perplexity.ai/hub/blog/introducing-model-council)

## Funcionalidades
- **M√∫ltiplos Modelos**: Selecione e consulte v√°rios modelos Ollama simultaneamente.
- **RAG Local**: Upload de PDF, DOCX ou TXT para usar como contexto.
- **S√≠ntese Inteligente**: Um modelo "Juiz" consolida as respostas, destacando consensos e diverg√™ncias.
- **Execu√ß√£o Paralela**: Respostas r√°pidas usando `asyncio`.
- **Interface Amig√°vel**: Constru√≠da com Streamlit.

## Pr√©-requisitos
1. **Python 3.9+** instalado.
2. **Ollama** instalado e rodando.
   - Baixe em: [ollama.com](https://ollama.com)
   - Certifique-se de ter baixado alguns modelos (ex: `ollama pull llama3`, `ollama pull mistral`, `ollama pull gemma`).

## Instala√ß√£o

1. Clone o reposit√≥rio ou navegue at√© a pasta:
   ```bash
   cd model_council_app
   ```

2. Crie um ambiente virtual (recomendado):
   ```bash
   python -m venv venv
   # Windows
   .\venv\Scripts\activate
   # Linux/Mac
   source venv/bin/activate
   ```

3. Instale as depend√™ncias:
   ```bash
   pip install -r requirements.txt
   ```

## Como Usar

1. Inicie a aplica√ß√£o:
   ```bash
   streamlit run app.py
   ```

2. O navegador abrir√° automaticamente (geralmente em `http://localhost:8501`).

3. **Na barra lateral**:
   - Verifique se os modelos foram carregados corretamente.
   - Selecione os modelos que far√£o parte do conselho.
   - Selecione o modelo "Juiz".
   - (Opcional) Fa√ßa upload de um documento para contexto.

4. **Na √°rea principal**:
   - Digite sua pergunta/prompt.
   - Clique em "Convening Council".

5. Aguarde as respostas individuais e a s√≠ntese final.

## Estrutura do Projeto

- `app.py`: Interface do usu√°rio (Streamlit).
- `council.py`: L√≥gica de orquestra√ß√£o e chamada aos modelos (Ollama).
- `rag.py`: Processamento de documentos e banco vetorial (ChromaDB + SentenceTransformers).
- `config.py`: Configura√ß√µes globais.

## Troubleshooting

- **Erro de conex√£o Ollama**: Certifique-se de que o aplicativo Ollama est√° rodando em background (`ollama serve` ou via aplicativo desktop).
- **Modelos n√£o aparecem**: Rode `ollama list` no terminal para garantir que voc√™ tem modelos baixados.
- **Erro no ChromaDB**: Se houver problemas com sqlite3, tente atualizar o pip ou instalar as build tools do C++.
